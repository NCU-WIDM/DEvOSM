{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 15 15:48:18 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |                  N/A |\n",
      "| 47%   36C    P2   107W / 350W |   9225MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "DEvOSM.ipynb\t\t   NN_model.ipynb  config.ini\n",
      "DEvOSM_ETL.ipynb\t   NN_model.py\t   feature_extractor.ipynb\n",
      "DEvOSM_verify.ipynb\t   VoteRule.ipynb  feature_extractor.py\n",
      "DP_set_matching_api.ipynb  VoteRule.py\t   requirements.txt\n",
      "DP_set_matching_api.py\t   __pycache__\t   schema_matching_data\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import neighbors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pymongo import MongoClient\n",
    "import configparser\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "\n",
    "from feature_extractor import *\n",
    "from VoteRule import *\n",
    "from NN_model import *\n",
    "from DP_set_matching_api import *\n",
    "\n",
    "# load config file data\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "client = MongoClient(f\"mongodb://{config['MongoDB']['account']}:{config['MongoDB']['password']}@{config['MongoDB']['ip']}:{config['MongoDB']['port']}/\")\n",
    "database = client[config['MongoDB']['database']]\n",
    "collection = database[config['MongoDB']['collection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['u53k9144jel6xdnnm8', 'u53k9144i0l6xdn5a1', 'u53k9144itl6usu36e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u53k9144i0l6xdn5a1\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "{1: [1], 2: [2], 3: [3], 4: [4], 5: [0], 6: [0]}\n",
      "same col\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "done\n",
      "same col\n",
      "[0, 1, 2]\n",
      "done\n",
      "same col\n",
      "[0, 1]\n",
      "done\n",
      "diff col 245   253\n",
      "u53k9144itl6usu36e\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "delete\n",
      "{1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 0: [6], 6: [7]}\n",
      "same col\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "done\n",
      "same col\n",
      "[0, 1, 2]\n",
      "done\n",
      "same col\n",
      "[0, 1]\n",
      "done\n",
      "diff col 245   229\n",
      "diff col 188   241\n",
      "same col\n",
      "[0]\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#nonset matching   \\n# remove set data from html\\n# read set data\\ndata = col_result[\\'setsData\\'] # set data\\n#open orig html\\n\\nfs=codecs.open(\"./nonsetdata/test/0000.html\", \\'r\\')\\nsoup = BeautifulSoup(fs, \\'lxml\\')\\n#delete tag\\nfor dim1 in data:\\n    for dim2 in dim1:\\n        for dim3 in dim2:\\n            print(dim3)\\n            try:\\n                for replace_ in soup.findAll(text=dim3):\\n                    #print(replace_)\\n                    replace_.replace_with(replace_.replace(dim3,\"\"))\\n                    #(replace_.parent).decompose()\\n            except:\\n                continue\\nfor x in soup.find_all():\\n    if len(x.get_text(strip=True)) == 0:\\n        print(x.extract())\\n#save to new html \\nwith open(\"save/to/new/html/0000.html\", \"w\") as file:\\n    file.write(str(soup))\\n#將新存好的html 去跑DCADE \\n#DCADE 要去呼叫jar\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "train = col_temp.find_one({'serialNumber': data[0]}) \n",
    "\n",
    "\n",
    "for ser_index in range(0,len(data)-1):\n",
    "    print(data[ser_index+1])\n",
    "\n",
    "    predict = col_temp.find_one({'serialNumber':data[ser_index+1]})\n",
    "    if ser_index == 0:\n",
    "#         try:\n",
    "#             set_result,train_index,predict_index = set_matching_api_forDB(train,predict) #set matching\n",
    "#             col_result = col_matching_forDB(set_result,train,predict,train_index,predict_index,model_select=1) #col matching\n",
    "#         except: \n",
    "#             col_result = \"None\"\n",
    "#             break;\n",
    "        set_result,train_index,predict_index = set_matching_api_forDB(train,predict) #set matching\n",
    "        col_result = col_matching_forDB(set_result,train,predict,train_index,predict_index,model_select=1) #col matching\n",
    "    else:\n",
    "        set_result2,train_index2,predict_index2 = set_matching_api_forDB(col_result,predict) #set matching\n",
    "        col_result = col_matching_forDB(set_result2,col_result,predict,train_index2,predict_index2,model_select=1)#col matching\n",
    "\n",
    "\n",
    "set_count = 0\n",
    "## 以下將<3的set排除掉，並寫到txt檔 對齊結果會存在col_result['setsData'] 裡面\n",
    "# try:\n",
    "#     pop_count = 0\n",
    "#     pop_index = []\n",
    "#     for index,col in enumerate(col_result['setsData']):\n",
    "#         if len(col)<3:\n",
    "#             pop_index.append(index)\n",
    "#     print(pop_index)\n",
    "#     for i in pop_index:\n",
    "#         col_result['setsData'].pop(i-pop_count)\n",
    "#         pop_count+=1\n",
    "# ### 排除完<3的\n",
    "# ### 把結果寫到txt\n",
    "#     for i in col_result['setsData']:\n",
    "        \n",
    "#         newpath = './schema_matching_data/multipage/test/web'+str(count)\n",
    "#         if not os.path.exists(newpath):\n",
    "#             os.makedirs(newpath)\n",
    "#         for j in i :\n",
    "#             print(j, file=open(newpath+'/set'+str(set_count)+\".txt\", \"a\"))\n",
    "#         set_count+=1\n",
    "# #         print(i)\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     newpath = './schema_matching_data/multipage/test/web'+str(count)\n",
    "#     if not os.path.exists(newpath):\n",
    "#         os.makedirs(newpath)\n",
    "       \n",
    "pop_count = 0\n",
    "pop_index = []\n",
    "for index,col in enumerate(col_result['setsData']):\n",
    "    if len(col)<3:\n",
    "        pop_index.append(index)\n",
    "for i in pop_index:\n",
    "    col_result['setsData'].pop(i-pop_count)\n",
    "    pop_count+=1\n",
    "### 排除完<3的\n",
    "### 把結果寫到txt\n",
    "for i in col_result['setsData']:\n",
    "\n",
    "    newpath = './schema_matching_data/multipage/test/web'+str(count)\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for j in i :\n",
    "        print(j, file=open(newpath+'/set'+str(set_count)+\".txt\", \"a\"))\n",
    "    set_count+=1\n",
    "#         print(i)\n",
    "        \n",
    "count+=1\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "#nonset matching   \n",
    "# remove set data from html\n",
    "# read set data\n",
    "data = col_result['setsData'] # set data\n",
    "#open orig html\n",
    "\n",
    "fs=codecs.open(\"./nonsetdata/test/0000.html\", 'r')\n",
    "soup = BeautifulSoup(fs, 'lxml')\n",
    "#delete tag\n",
    "for dim1 in data:\n",
    "    for dim2 in dim1:\n",
    "        for dim3 in dim2:\n",
    "            print(dim3)\n",
    "            try:\n",
    "                for replace_ in soup.findAll(text=dim3):\n",
    "                    #print(replace_)\n",
    "                    replace_.replace_with(replace_.replace(dim3,\"\"))\n",
    "                    #(replace_.parent).decompose()\n",
    "            except:\n",
    "                continue\n",
    "for x in soup.find_all():\n",
    "    if len(x.get_text(strip=True)) == 0:\n",
    "        print(x.extract())\n",
    "#save to new html \n",
    "with open(\"save/to/new/html/0000.html\", \"w\") as file:\n",
    "    file.write(str(soup))\n",
    "#將新存好的html 去跑DCADE \n",
    "#DCADE 要去呼叫jar\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
