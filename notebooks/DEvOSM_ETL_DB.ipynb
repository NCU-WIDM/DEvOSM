{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2NvdW50Ijoid2lkbS5kZXZvc21AZ21haWwuY29tIiwiaWF0IjoxNjczODgyMTQ0LCJleHAiOjUyNzM4Nzg1NDR9.aeS8Pr8iFdq7roXbGLInPSs9BrwAidY7xFcFqLE6qnE'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "# load config file data\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "# login to get jwt token\n",
    "\n",
    "login_data = {'account': config['ETL']['account'], 'password': config['ETL']['password']}\n",
    "res = requests.post('http://140.115.54.44:8001/api/auth/login', json=login_data)\n",
    "token = json.loads(res.text)['token']\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genarate SingleList Extractor (MDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls = ['https://www.com.tw/cross/check_016222_NO_1_111_0_3.html', 'https://www.com.tw/cross/check_004442_NO_1_111_0_3.html', 'https://www.com.tw/cross/check_001582_NO_0_111_0_3.html']\n",
    "# sample_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u53k97exvlcywj94p'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = { 'Authorization' : 'Bearer ' + token}\n",
    "body = {\n",
    "    'name': 'DEvOSM_' + str(datetime.now().strftime('%y%m%d%H%M%S%f')),\n",
    "    'dataSource': 'puppeteer',\n",
    "    'pageType': 'DEvOSM',\n",
    "    'source':{\n",
    "        'params': [{'name': 'autoGenerate', 'type': 'stringList', 'range': [1, 1], 'stringListSource': 'empty', 'stringList': sample_urls }],\n",
    "        'pattern': '${autoGenerate}'\n",
    "    },\n",
    "    'urls': sample_urls,\n",
    "    'updateTime': 0,\n",
    "    'waitTime': 15,\n",
    "    'option': {\n",
    "        'dcadeMerge': False\n",
    "    },\n",
    "    'UseCache': False,\n",
    "}\n",
    "\n",
    "res = requests.post('http://140.115.54.44:8001/api/extractors/create', json=body, headers=headers)\n",
    "serial_number = json.loads(res.text)['serialNumber']\n",
    "serial_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Response [200]>, 'u53k9cdiflcyy7mkq')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_urls = ['https://www.books.com.tw/web/sys_bbotm/books/010101/?o=1&v=1&page=6', 'https://www.books.com.tw/web/sys_bbotm/books/010101/?o=1&v=1&page=7']\n",
    "\n",
    "headers = {'Authorization': 'Bearer ' + token}\n",
    "body = {\n",
    "    'dataSource': 'puppeteer',\n",
    "    'urls': sample_urls,\n",
    "    'updateTime': 0,\n",
    "}\n",
    "\n",
    "res = requests.post(\n",
    "    'http://140.115.54.44:8001/api/extractors/re-extract-by-devosm/u53k97ex3lcywlbtf', json=body, headers=headers)\n",
    "serial_number = json.loads(res.text)['serialNumber']\n",
    "res, serial_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database for Extractor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(f\"mongodb://{config['MongoDB']['account']}:{config['MongoDB']['password']}@{config['MongoDB']['ip']}:{config['MongoDB']['port']}/\")\n",
    "database = client[config['MongoDB']['database']]\n",
    "collection = database[config['MongoDB']['collection']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Multiple Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_serial_numbers = ['u53k9144jel6xdnnm8', 'u53k9144i0l6xdn5a1', 'u53k9144itl6usu36e']\n",
    "serial_numbers = sample_serial_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** u53k9144i0l6xdn5a1 **********\n",
      "--set matching--\n",
      "- master -\n",
      "Set_1 data less than 3. Skip this set!\n",
      "Set_2 data less than 3. Skip this set!\n",
      "Set_3 data less than 3. Skip this set!\n",
      "Set_4 data less than 3. Skip this set!\n",
      "Set_5 data less than 3. Skip this set!\n",
      "Set_10 data less than 3. Skip this set!\n",
      "Set_11 data less than 3. Skip this set!\n",
      "Set_13 data less than 3. Skip this set!\n",
      "- slave -\n",
      "Set_1 data less than 3. Skip this set!\n",
      "Set_2 data less than 3. Skip this set!\n",
      "Set_3 data less than 3. Skip this set!\n",
      "Set_4 data less than 3. Skip this set!\n",
      "Set_5 data less than 3. Skip this set!\n",
      "Set_6 data less than 3. Skip this set!\n",
      "--col matching--\n",
      "{1: [1], 2: [2], 3: [3], 4: [4], 5: [0], 6: [0]}\n",
      "same col\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "done\n",
      "same col\n",
      "[0, 1, 2]\n",
      "done\n",
      "same col\n",
      "[0, 1]\n",
      "done\n",
      "diff col 245   253\n",
      "********** u53k9144itl6usu36e **********\n",
      "--set matching--\n",
      "- master -\n",
      "Set_1 data less than 3. Skip this set!\n",
      "Set_2 data less than 3. Skip this set!\n",
      "Set_3 data less than 3. Skip this set!\n",
      "Set_4 data less than 3. Skip this set!\n",
      "Set_5 data less than 3. Skip this set!\n",
      "Set_10 data less than 3. Skip this set!\n",
      "Set_11 data less than 3. Skip this set!\n",
      "Set_13 data less than 3. Skip this set!\n",
      "- slave -\n",
      "Set_1 data less than 3. Skip this set!\n",
      "Set_2 data less than 3. Skip this set!\n",
      "Set_3 data less than 3. Skip this set!\n",
      "Set_4 data less than 3. Skip this set!\n",
      "Set_5 data less than 3. Skip this set!\n",
      "Set_9 data less than 3. Skip this set!\n",
      "Set_10 data less than 3. Skip this set!\n",
      "Set_13 data less than 3. Skip this set!\n",
      "Set_14 data less than 3. Skip this set!\n",
      "Set_15 data less than 3. Skip this set!\n",
      "Set_17 data less than 3. Skip this set!\n",
      "--col matching--\n",
      "{1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 0: [6], 6: [7]}\n",
      "same col\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "done\n",
      "same col\n",
      "[0, 1, 2]\n",
      "done\n",
      "same col\n",
      "[0, 1]\n",
      "done\n",
      "diff col 245   229\n",
      "diff col 188   241\n",
      "same col\n",
      "[0]\n",
      "done\n",
      "3.79 s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    " \n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from util.matching import *\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "folder_path = '../schema_matching_data/multipage/test/Web_' + str(datetime.now().strftime('%y%m%d%H%M%S'))\n",
    "master = collection.find_one({'serialNumber': serial_numbers[0]})['setsData']\n",
    "\n",
    "# start matching data\n",
    "if len(serial_numbers) > 1:\n",
    "    for serial_number in serial_numbers[1:]:\n",
    "        print('*'*10, serial_number, '*'*10)\n",
    "\n",
    "        slave = collection.find_one({'serialNumber': serial_number})['setsData']\n",
    "        # set matching\n",
    "        print('--set matching--')\n",
    "        set_result, master_index, slave_index = sets_matching(master, slave)\n",
    "        # col matching\n",
    "        print('--col matching--')\n",
    "        master = col_matching_forDB(set_result, master, slave, master_index, slave_index, model_select=2)\n",
    "else:\n",
    "    print(\"Unable to combine!!!\")\n",
    "\n",
    "sets_data = master\n",
    "\n",
    "# remove sets_data less than 3\n",
    "for index, data in enumerate(sets_data.copy()):\n",
    "    if len(data) < 3:\n",
    "        sets_data.remove(data)\n",
    "\n",
    "# check if folder path exists\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "for set_index, set_data in enumerate(sets_data):\n",
    "    with open(f\"{folder_path}/set_{str(set_index)}.txt\", 'w') as fs:\n",
    "        for col_data in set_data:\n",
    "            fs.write(str(col_data) + '\\n')\n",
    "            \n",
    "end = time.time()\n",
    "print(\"{:.2f} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#nonset matching   \\n# remove set data from html\\n# read set data\\ndata = col_result[\\'setsData\\'] # set data\\n#open orig html\\n\\nfs=codecs.open(\"./nonsetdata/test/0000.html\", \\'r\\')\\nsoup = BeautifulSoup(fs, \\'lxml\\')\\n#delete tag\\nfor dim1 in data:\\n    for dim2 in dim1:\\n        for dim3 in dim2:\\n            print(dim3)\\n            try:\\n                for replace_ in soup.findAll(text=dim3):\\n                    #print(replace_)\\n                    replace_.replace_with(replace_.replace(dim3,\"\"))\\n                    #(replace_.parent).decompose()\\n            except:\\n                continue\\nfor x in soup.find_all():\\n    if len(x.get_text(strip=True)) == 0:\\n        print(x.extract())\\n#save to new html \\nwith open(\"save/to/new/html/0000.html\", \"w\") as file:\\n    file.write(str(soup))\\n#將新存好的html 去跑DCADE \\n#DCADE 要去呼叫jar\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#nonset matching   \n",
    "# remove set data from html\n",
    "# read set data\n",
    "data = col_result['setsData'] # set data\n",
    "#open orig html\n",
    "\n",
    "fs=codecs.open(\"./nonsetdata/test/0000.html\", 'r')\n",
    "soup = BeautifulSoup(fs, 'lxml')\n",
    "#delete tag\n",
    "for dim1 in data:\n",
    "    for dim2 in dim1:\n",
    "        for dim3 in dim2:\n",
    "            print(dim3)\n",
    "            try:\n",
    "                for replace_ in soup.findAll(text=dim3):\n",
    "                    #print(replace_)\n",
    "                    replace_.replace_with(replace_.replace(dim3,\"\"))\n",
    "                    #(replace_.parent).decompose()\n",
    "            except:\n",
    "                continue\n",
    "for x in soup.find_all():\n",
    "    if len(x.get_text(strip=True)) == 0:\n",
    "        print(x.extract())\n",
    "#save to new html \n",
    "with open(\"save/to/new/html/0000.html\", \"w\") as file:\n",
    "    file.write(str(soup))\n",
    "#將新存好的html 去跑DCADE \n",
    "#DCADE 要去呼叫jar\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
