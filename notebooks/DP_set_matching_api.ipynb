{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def _cosine(a,b):\n",
    "    ans = (1 - spatial.distance.cosine(a,b))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def bertembedding(df):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    list_data = []\n",
    "    for column in df:\n",
    "        list_data+=df[column].tolist()\n",
    "\n",
    "    sentences = list_data\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DP_matching(set_rep_l,set_p_rep_l,window_size):\n",
    "#set1:　　　　　　　set_rep_l\n",
    "#set2:             set_p_rep_l\n",
    "    set1_result = []\n",
    "    set2_result = []\n",
    "    set1_stop_len = 0\n",
    "    set2_stop_len = 0 #紀錄當前走多少\n",
    "    finish = max(len(set_rep_l),len(set_p_rep_l))\n",
    "    set1_finish =  finish\n",
    "    set2_finish = finish#終止條件\n",
    "\n",
    "    set1_index = 0\n",
    "    set2_index = 0\n",
    "    while set1_stop_len<set1_finish and  set2_stop_len<set2_finish:\n",
    "        temp = []  \n",
    "        #print(\"set1_index:\",set1_index)\n",
    "        #print(\"set2_index:\",set2_index)\n",
    "        try: #0\n",
    "            temp.append(_cosine(set_rep_l[set1_index],set_p_rep_l[set2_index]))\n",
    "            #temp.append(1 - spatial.distance.cosine(set_rep_l[set1_index], set_p_rep_l[set2_index]))\n",
    "        except:\n",
    "            temp.append(0)\n",
    "        for size in range(1,window_size):\n",
    "            try: #1\n",
    "                temp.append(_cosine(set_rep_l[set1_index+size],set_p_rep_l[set2_index]))\n",
    "                #temp.append(1 - spatial.distance.cosine(set_rep_l[set1_index+1], set_p_rep_l[set2_index]))\n",
    "            except:\n",
    "                temp.append(0)\n",
    "\n",
    "        for size in range(1,window_size):          \n",
    "            try: #3\n",
    "                temp.append(_cosine(set_rep_l[set1_index],set_p_rep_l[set2_index+size]))\n",
    "                #temp.append(1 - spatial.distance.cosine(set_rep_l[set1_index], set_p_rep_l[set2_index+1]))\n",
    "            except:\n",
    "                temp.append(0) \n",
    "              \n",
    "                \n",
    "        #print(temp)\n",
    "        #     A'    B'    C'  D'   E'(set1)\n",
    "        #  A  0     1     2   ...  w\n",
    "        #  B  w+1   w'+1  1'  ...  w'\n",
    "        #  C  w+2   w'+2  0'' ...  w'' \n",
    "        #  D  w+3   w'+3  3''\n",
    "        #  E              \n",
    "        #(set2)   \n",
    "        #print(temp)\n",
    "        max_value = max(temp)\n",
    "        max_index = temp.index(max_value)\n",
    "        #print('max_index:',max_index)\n",
    "        if temp[0] >0.95:\n",
    "            if set1_index < len(set_rep_l):\n",
    "                    set1_result.append(set1_index)\n",
    "            else:\n",
    "                set1_result.append(-1)\n",
    "            if set2_index < len(set_p_rep_l):\n",
    "                set2_result.append(set2_index)\n",
    "            else:\n",
    "                set2_result.append(-1)    \n",
    "            set1_index +=1\n",
    "            set2_index +=1\n",
    "            set1_stop_len +=1\n",
    "            set2_stop_len +=1\n",
    "            continue       \n",
    "        if max_index == 0:\n",
    "                if set1_index < len(set_rep_l):\n",
    "                    set1_result.append(set1_index)\n",
    "                else:\n",
    "                    set1_result.append(-1)\n",
    "                if set2_index < len(set_p_rep_l):\n",
    "                    set2_result.append(set2_index)\n",
    "                else:\n",
    "                    set2_result.append(-1)    \n",
    "                set1_index +=1\n",
    "                set2_index +=1\n",
    "                set1_stop_len +=1\n",
    "                set2_stop_len +=1\n",
    "        elif max_index<window_size:    \n",
    "            compare_cosine = []\n",
    "            for size in range(1,window_size):\n",
    "                try: #1\n",
    "                    C1 = _cosine(set_rep_l[set1_index+max_index],set_p_rep_l[set2_index+size])\n",
    "                    #C1 = 1 - spatial.distance.cosine(set_rep_l[set1_index+1], set_p_rep_l[set2_index+1])\n",
    "                except:\n",
    "                    C1 = 0\n",
    "                compare_cosine.append(C1)\n",
    "\n",
    "            #print(' C1:', C1,'\\nC2:', C2)\n",
    "            if all( max_value >= checkvalue  for checkvalue in compare_cosine)and max_value>=0.5:\n",
    "                for num in range(max_index):\n",
    "                     set2_result.append(-1)\n",
    "\n",
    "\n",
    "                if set1_index < finish:\n",
    "#                         set1_result.append(set1_index)\n",
    "                    for num in range(max_index+1):\n",
    "                         set1_result.append(set1_index+num)               \n",
    "                else:\n",
    "                    set1_result.append(-1)\n",
    "\n",
    "\n",
    "                if set2_index <finish:\n",
    "                    set2_result.append(set2_index)\n",
    "                else:\n",
    "                    set2_result.append(-1)   \n",
    "                set1_index +=(1+max_index)\n",
    "                set2_index +=1\n",
    "                set1_stop_len +=(1+max_index)\n",
    "                set2_stop_len +=1\n",
    "                set1_finish += (1+max_index-1)                  \n",
    "            else:\n",
    "                set1_result.append(set1_index)\n",
    "                set2_result.append(-1)\n",
    "                set1_index +=1\n",
    "        elif max_index>=window_size:\n",
    "            compare_cosine = []\n",
    "            for size in range(1,window_size):\n",
    "                try: #1\n",
    "                    C1 = _cosine(set_rep_l[set1_index+size],set_p_rep_l[set2_index+max_index])\n",
    "                    #C1 = 1 - spatial.distance.cosine(set_rep_l[set1_index+1], set_p_rep_l[set2_index+1])\n",
    "                except:\n",
    "                    C1 = 0\n",
    "                compare_cosine.append(C1)\n",
    "\n",
    "            #print(' C1:', C1,'\\nC2:', C2)\n",
    "            if all( max_value >= checkvalue  for checkvalue in compare_cosine)and max_value>=0.5:\n",
    "                for num in range(max_index-window_size+1):\n",
    "                     set1_result.append(-1)\n",
    "\n",
    "                #print(\"長度:\",len(set_rep_l),\"set2index:\",set2_index)\n",
    "                if set2_index < finish:\n",
    "#                         set1_result.append(set1_index)\n",
    "                    for num in range(max_index+1-window_size+1):\n",
    "                         #print(\"set2 add :\", set2_index+num)\n",
    "                         set2_result.append(set2_index+num)               \n",
    "                else:\n",
    "                    set2_result.append(-1)\n",
    "\n",
    "\n",
    "                if set1_index < finish:\n",
    "                    set1_result.append(set1_index)\n",
    "                else:\n",
    "                    set1_result.append(-1) \n",
    "                    \n",
    "                set1_index +=1\n",
    "                set2_index +=(1+max_index-window_size+1)\n",
    "                set1_stop_len +=1\n",
    "                set2_stop_len +=(1+max_index-window_size+1)\n",
    "                set2_finish += (1+max_index-1-window_size+1)                  \n",
    "            else:\n",
    "                set2_result.append(set2_index)\n",
    "                set1_result.append(-1)\n",
    "                set2_index +=1\n",
    "        #print(set1_result)\n",
    "        #print(set2_result)\n",
    "        \"\"\"\n",
    "        print('stop1:',set1_stop_len)\n",
    "        print('stop2:',set2_stop_len)\n",
    "        print('finish1',set1_finish)\n",
    "        print('finish2',set2_finish)\n",
    "        print(set1_result)\n",
    "        print(set2_result)\n",
    "        \"\"\"   \n",
    "    set1_result = [x+1 for x in set1_result]\n",
    "    set2_result = [x+1 for x in set2_result]\n",
    "#     print(set1_result)\n",
    "#     print(set2_result)\n",
    "    while set1_result[-1] == 0 and set2_result[-1]==0:\n",
    "        set1_result.pop()\n",
    "        set2_result.pop()\n",
    "    return  set1_result, set2_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_embedding(serialnumber):\n",
    "    train = col_temp.find_one({'serialNumber': serialnumber}) #weir Exalg\n",
    "    set_rep = []\n",
    "    #print(page)\n",
    "    #start 處理 train data\n",
    "    count_col = 0\n",
    "    for sets_train in train['setsData']:       \n",
    "        arr_t = np.array(sets_train)\n",
    "        df = pd.DataFrame(arr_t, columns=['col'+str(item) for item in range(0,len(arr_t.T))])  \n",
    "        row_count = 0\n",
    "        data_train = []\n",
    "        for i, row in df.iterrows():\n",
    "            data_temp = \"\"\n",
    "            for j, column in row.iteritems():\n",
    "                data_temp = data_temp + column\n",
    "            data_train.append(data_temp)\n",
    "            row_count+=1\n",
    "#         if row_count <= 3:\n",
    "#             print(\"delete\")\n",
    "#             continue;\n",
    "            \n",
    "            \n",
    "        d = {'text': data_train}\n",
    "        row_df = pd.DataFrame(data=d)\n",
    "        row_embedding = bertembedding(row_df)        \n",
    "        row_embedding_np = np.matrix(row_embedding)\n",
    "        avg_row_embedding = row_embedding_np.sum(axis=0)/len(row_embedding_np)\n",
    "        set_rep.append(avg_row_embedding)\n",
    "        count_col +=1\n",
    "    return set_rep,train,count_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_matching_api(master,slave):\n",
    "    set_rep = []\n",
    "    #print(page)\n",
    "    #start 處理 train data\n",
    "    set_rep,train,count_col_m = set_embedding(master)\n",
    "    #for serialNumber in range(2,len(page)): #dacde failed\n",
    "    set_p_rep,predict,count_col_s = set_embedding(slave)\n",
    "    set_rep_l = []\n",
    "    set_p_rep_l = []\n",
    "    \n",
    "    for x in set_rep:\n",
    "        set_rep_l+=x.tolist()\n",
    "    for x in set_p_rep:\n",
    "        set_p_rep_l+=x.tolist()\n",
    "    #print(page[serialNumber])\n",
    "    #print('set_rep_l:',len(set_rep_l),'set_p_rep_l:',len(set_p_rep_l))\n",
    "    #print(set_rep_l)\n",
    "    \n",
    "    set_d = len(train['setsData']) - len(predict['setsData'])\n",
    "    #print(abs(set_d))\n",
    "    window_size = abs(set_d) + 5\n",
    "    print(\"len(train['setsData']):\",len(train['setsData']))\n",
    "    print(\"len(predict['setsData']:\",len(predict['setsData']))\n",
    "    print(\"window_size:\", window_size)\n",
    "    \n",
    "    print(\"count_col_m:\",count_col_m)\n",
    "    print(\"count_col_s:\",count_col_s)\n",
    "    set_d2 = count_col_m - count_col_s\n",
    "    window_size2 = abs(set_d2) + 5\n",
    "    print(\"window_size2:\", window_size2)\n",
    "    \n",
    "    if count_col_m ==0 and count_col_s ==0:\n",
    "        dict ={}\n",
    "        return dict\n",
    "    else:\n",
    "        set1_result,set2_result=DP_matching(set_rep_l,set_p_rep_l,window_size2)\n",
    "#         print(page[serialNumber], file=open(\"./schema_matching_data/set_result/Exalg/testebat\"+str(countpage)+\".txt\", \"a\"))\n",
    "#         print(set1_result, file=open(\"./schema_matching_data/set_result/Exalg/testebat\"+str(countpage)+\".txt\", \"a\"))\n",
    "#         print(set2_result, file=open(\"./schema_matching_data/set_result/Exalg/testebat\"+str(countpage)+\".txt\", \"a\"))\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dict ={} #用來處理多Page一起合併的結果\n",
    "    first_merge = True #用來判斷是否為第一次合併\n",
    "    collect_no_match_vec = [] #用來收集對齊到空的那些set的向量表示\n",
    "    collect_no_match_num = [] #用來表示每個set有幾個對齊到空的數量 , for example, [2,3,4] 表示page1 有兩個set對到空 page2有3個...\n",
    "    \n",
    "    count_page1_no_match = 0\n",
    "    count_page2_no_match = 0\n",
    "    for i in range(len(set2_result)):\n",
    "        if first_merge:\n",
    "            if set1_result[i] in dict:\n",
    "                #print(set1_result[i])\n",
    "                dict.get(set1_result[i]).append(set2_result[i])\n",
    "            else:\n",
    "                temp = [set2_result[i]]            \n",
    "                dict[set1_result[i]] = temp\n",
    "        else:\n",
    "            if set1_result[i] in dict:\n",
    "                #print(set1_result[i])\n",
    "                dict.get(set1_result[i]).append(set2_result[i])\n",
    "\n",
    "        if set1_result[i] == 0: #表示page2的set2_result[i] 對齊到空集 要將資料額外取出來 最後再統整時重算相似度\n",
    "                collect_no_match_vec.append(set_p_rep_l[set2_result[i]-1]) #page2的set2_result[i] 的向量表示, set2_result[i]表示在page2第i個set\n",
    "                count_page2_no_match +=1\n",
    "\n",
    "    collect_no_match_num.append(count_page2_no_match)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"testing\",master,\"and set1_result:\",set1_result)\n",
    "    print(\"testing\",slave,\"and set2_result:\",set2_result)\n",
    "    return dict\n",
    "  \n",
    "        #print(\"testing\",page[serialNumber],\"and set1_result:\",set1_result)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_embedding_forDB(train):\n",
    "    set_rep = []\n",
    "    #print(page)\n",
    "    #start 處理 train data\n",
    "    train_index = [] # 表示在資料庫形式中，資料儲存的位置(index)\n",
    "    train_i = 0\n",
    "    count_col = 0\n",
    "    for sets_train in train['setsData']:       \n",
    "        arr_t = np.array(sets_train)\n",
    "        df = pd.DataFrame(arr_t, columns=['col'+str(item) for item in range(0,len(arr_t.T))])  \n",
    "        row_count = 0\n",
    "        data_train = []\n",
    "        for i, row in df.iterrows():\n",
    "            data_temp = \"\"\n",
    "            for j, column in row.iteritems():\n",
    "                data_temp = data_temp + column\n",
    "            data_train.append(data_temp)\n",
    "            row_count+=1\n",
    "        if row_count <= 3:\n",
    "            train_i +=1 \n",
    "            print(\"delete\")\n",
    "            continue;\n",
    "        train_index.append(train_i)\n",
    "        train_i +=1 \n",
    "        d = {'text': data_train}\n",
    "        row_df = pd.DataFrame(data=d)\n",
    "        row_embedding = bertembedding(row_df)        \n",
    "        row_embedding_np = np.matrix(row_embedding)\n",
    "        avg_row_embedding = row_embedding_np.sum(axis=0)/len(row_embedding_np)\n",
    "        set_rep.append(avg_row_embedding)\n",
    "        count_col +=1\n",
    "    return set_rep,train,count_col,train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_matching_api_forDB(master,slave):\n",
    "    set_rep = []\n",
    "    #print(page)\n",
    "    #start 處理 train data\n",
    "    set_rep,train,count_col_m,train_index = set_embedding_forDB(master)\n",
    "    #for serialNumber in range(2,len(page)): #dacde failed\n",
    "    set_p_rep,predict,count_col_s,predict_index = set_embedding_forDB(slave)\n",
    "    set_rep_l = []\n",
    "    set_p_rep_l = []\n",
    "    \n",
    "    for x in set_rep:\n",
    "        set_rep_l+=x.tolist()\n",
    "    for x in set_p_rep:\n",
    "        set_p_rep_l+=x.tolist()\n",
    "    #print(page[serialNumber])\n",
    "    #print('set_rep_l:',len(set_rep_l),'set_p_rep_l:',len(set_p_rep_l))\n",
    "    #print(set_rep_l)\n",
    "    \n",
    "    set_d = len(train['setsData']) - len(predict['setsData'])\n",
    "    #print(abs(set_d))\n",
    "    window_size = abs(set_d) + 5\n",
    "    #print(\"len(train['setsData']):\",len(train['setsData']))\n",
    "    #print(\"len(predict['setsData']:\",len(predict['setsData']))\n",
    "    #print(\"window_size:\", window_size)\n",
    "    \n",
    "    #print(\"count_col_m:\",count_col_m)\n",
    "    #print(\"count_col_s:\",count_col_s)\n",
    "    set_d2 = count_col_m - count_col_s\n",
    "    window_size2 = abs(set_d2) + 5\n",
    "    #print(\"window_size2:\", window_size2)\n",
    "    \n",
    "    if count_col_m ==0 and count_col_s ==0:\n",
    "        dict ={}\n",
    "        return dict\n",
    "    else:\n",
    "        set1_result,set2_result=DP_matching(set_rep_l,set_p_rep_l,window_size2)\n",
    "#         print(page[serialNumber], file=open(\"./schema_matching_data/set_result/Exalg/testebat\"+str(countpage)+\".txt\", \"a\"))\n",
    "#         print(set1_result, file=open(\"./schema_matching_data/set_result/Exalg/testebat\"+str(countpage)+\".txt\", \"a\"))\n",
    "#         print(set2_result, file=open(\"./schema_matching_data/set_result/Exalg/testebat\"+str(countpage)+\".txt\", \"a\"))\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dict ={} #用來處理多Page一起合併的結果\n",
    "    first_merge = True #用來判斷是否為第一次合併\n",
    "    collect_no_match_vec = [] #用來收集對齊到空的那些set的向量表示\n",
    "    collect_no_match_num = [] #用來表示每個set有幾個對齊到空的數量 , for example, [2,3,4] 表示page1 有兩個set對到空 page2有3個...\n",
    "    \n",
    "    count_page1_no_match = 0\n",
    "    count_page2_no_match = 0\n",
    "    for i in range(len(set2_result)):\n",
    "        if first_merge:\n",
    "            if set1_result[i] in dict:\n",
    "                #print(set1_result[i])\n",
    "                dict.get(set1_result[i]).append(set2_result[i])\n",
    "            else:\n",
    "                temp = [set2_result[i]]            \n",
    "                dict[set1_result[i]] = temp\n",
    "        else:\n",
    "            if set1_result[i] in dict:\n",
    "                #print(set1_result[i])\n",
    "                dict.get(set1_result[i]).append(set2_result[i])\n",
    "\n",
    "        if set1_result[i] == 0: #表示page2的set2_result[i] 對齊到空集 要將資料額外取出來 最後再統整時重算相似度\n",
    "                collect_no_match_vec.append(set_p_rep_l[set2_result[i]-1]) #page2的set2_result[i] 的向量表示, set2_result[i]表示在page2第i個set\n",
    "                count_page2_no_match +=1\n",
    "\n",
    "    collect_no_match_num.append(count_page2_no_match)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #print(\"testing\",master,\"and set1_result:\",set1_result)\n",
    "    #print(\"testing\",slave,\"and set2_result:\",set2_result)\n",
    "    return dict, train_index, predict_index\n",
    "  \n",
    "        #print(\"testing\",page[serialNumber],\"and set1_result:\",set1_result)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_matching_forDB(set_result,train,predcit,train_index , predict_index,model_select=2):\n",
    "    #print(type(train))\n",
    "    print(set_result)\n",
    "    result = train.copy()\n",
    "    #print(type(result))\n",
    "    for master_index,slave_index in set_result.items():\n",
    "            #print(\"master_index:\",master_index,\"slave_index:\",slave_index)\n",
    "            #print(\"serialNumber_index:\",serialNumber_index-1)\n",
    "            #print(\"slave_index:\",slave_index[serialNumber_index-1])\n",
    "            #print(\"start set matching\")\n",
    "            #print(slave_index[serialNumber_index-1]-1)\n",
    "            if master_index ==0: \n",
    "                #no_matching_set += len(slave_index)\n",
    "                #print(\"master_index == 0 skip\")\n",
    "                for le in range(len(slave_index)):\n",
    "                    train.get('setsData').append(predict['setsData'][le])\n",
    "                continue\n",
    "            else:\n",
    "                if (slave_index[0]-1) == -1:\n",
    "                   # print(\"slave_index[0]-1 skip\")\n",
    "                    #no_matching_set += len(slave_index)\n",
    "                    continue\n",
    "                else:\n",
    "\n",
    "                    sets_train = train['setsData'][train_index[master_index-1]]\n",
    "                    #sets_test = predcit['setsData'][slave_index[serialNumber_index-1]-1]\n",
    "                    if (slave_index[0]-1) <0:\n",
    "                        print(\"slave_index[0]-1 <0 skip\",slave_index[0])\n",
    "                        continue;\n",
    "                    sets_test = predcit['setsData'][predict_index[slave_index[0]-1]]\n",
    "\n",
    "                    if len(sets_train[0]) == len(sets_test[0]):\n",
    "                        #print()\n",
    "                        print(\"same col\")\n",
    "                    else:\n",
    "                        print(\"diff col\",len(sets_train[0]),\" \",len(sets_test[0]))\n",
    "                        #error += 1\n",
    "                        continue;\n",
    "                    arr_t = np.array(sets_train)\n",
    "\n",
    "                    df = pd.DataFrame(arr_t, columns=['col'+str(item) for item in range(0,len(arr_t.T))])\n",
    "\n",
    "                    arr_p = np.array(sets_test)\n",
    "\n",
    "                    df_predict = pd.DataFrame(arr_p, columns=['col'+str(item) for item in range(0,len(arr_p.T))])    \n",
    "\n",
    "                    embeddings = extract(df)\n",
    "\n",
    "                    embeddings_predict = extract(df_predict)\n",
    "\n",
    "                    # #df\n",
    "\n",
    "\n",
    "                    label_ = []\n",
    "                    for j in range(0,len(df.columns)):\n",
    "                        for i in range(0,len(df)):\n",
    "                            label_.append(j)\n",
    "\n",
    "                    label_prdict = []\n",
    "                    for j in range(0,len(df_predict.columns)):\n",
    "                        for i in range(0,len(df_predict)):\n",
    "                            label_prdict.append(j)                    \n",
    "\n",
    "                    label_ = np.array(label_)\n",
    "                    #print(len(label_))\n",
    "                    label_prdict = np.array(label_prdict)\n",
    "                    #print(len(label_prdict))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    if model_select == 1:\n",
    "                        if 1 in label_:\n",
    "                            my_model = svm.SVC(probability=True) \n",
    "                            my_model = my_model.fit(embeddings, label_)\n",
    "                    elif model_select == 2: \n",
    "\n",
    "                        knn_clf = neighbors.KNeighborsClassifier(n_neighbors = len(df.columns))\n",
    "                        my_model = knn_clf.fit(embeddings, label_)\n",
    "                        y_predicted = my_model.predict(embeddings_predict)\n",
    "                        #print(y_predicted)\n",
    "                        #print(label_prdict)\n",
    "                    elif model_select == 3:\n",
    "                        learning_rate = 0.003\n",
    "                        #learning_rate = 0.001\n",
    "                        epochs = 8\n",
    "                        batch_size = 4\n",
    "                        validation_split = 0.1\n",
    "                        class_num = len(df.columns)\n",
    "                        my_model = create_model(learning_rate,class_num)\n",
    "\n",
    "                        # Train the model on the normalized training set.\n",
    "                        epochs, hist = train_model(my_model, embeddings, label_, epochs, batch_size, validation_split)\n",
    "\n",
    "                        # Plot a graph of the metric vs. epochs.\n",
    "                        list_of_metrics_to_plot = ['accuracy']\n",
    "                        #plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "                        # Evaluate against the test set.\n",
    "                        #print(\"\\n Evaluate the new model against the test set:\") \n",
    "                        #loss, accuracy, f1_score, precision, recall = my_model.evaluate(x=embeddings_predict, y=label_prdict, batch_size=batch_size)\n",
    "                        y_pred = my_model.predict(embeddings_predict)\n",
    "\n",
    "\n",
    "\n",
    "                column_num = len(df.columns)\n",
    "                testing_row_number = len(df_predict)\n",
    "                if model_select == 3 :\n",
    "                    group_vote = cal_vote_result_for_NN(my_model,column_num,testing_row_number,embeddings_predict)\n",
    "                    ans = [x for x in range(0,column_num)]\n",
    "                    column_result = classifier(column_num,group_vote)\n",
    "                else:\n",
    "                    if 1 in label_:\n",
    "                        group_vote = cal_vote_result_for_KNN(my_model,column_num,testing_row_number,embeddings_predict)\n",
    "                        ans = [x for x in range(0,column_num)]\n",
    "                        column_result = classifier(column_num,group_vote)\n",
    "                    else:\n",
    "                        ans = [0]         \n",
    "                        column_result = [0]\n",
    "                #print(ans)\n",
    "                #print(column_result)\n",
    "\n",
    "                #res = sum(x == y for x, y in zip(ans, column_result)) #計算當前set 正確的col數\n",
    "                #print(res)\n",
    "                #count_all_col_num_temp += len(ans)\n",
    "                #count_correct_num_temp += res\n",
    "                #Evaluation(ans,column_result,f1,precision,recall,accuracy_e,ans_list,column_result_list)\n",
    "                #ans_.append(x[serialNumber_index])\n",
    "                #ans_index.append(serialNumber_index)\n",
    "\n",
    "                #紀錄main set正確col數量\n",
    "                #if count_current_set_index == main:\n",
    "                #    main_count_all_col_num.append(len(ans)) \n",
    "                #    main_count_correct_num.append(res)\n",
    "                #    main_count_error_num.append(len(ans)-res)\n",
    "\n",
    "                #tmp = res/len(ans) #計算當前set準確度\n",
    "                #evl_avg_set += tmp\n",
    "\n",
    "                #count_current_set_index +=1\n",
    "                print(column_result)\n",
    "                #total_set_num +=1\n",
    "                \n",
    "                #print(column_result)\n",
    "                #print(predict['setsData'][slave_index[0]-1])\n",
    "                index = master_index-1\n",
    "                #print(result['setsData'][index])\n",
    "                for i in range(len(predict['setsData'][predict_index[slave_index[0]-1]])):\n",
    "                    #print(i)\n",
    "                    temp_arr = []\n",
    "                    \n",
    "                    for x in column_result:\n",
    "                        #print(slave_index[0]-1)\n",
    "                        #print(predict_index[slave_index[0]-1])\n",
    "                        #print('x:',x)\n",
    "                        #print('i:',i)\n",
    "                        #print(len(predict['setsData'][predict_index[slave_index[0]-1]][i]))\n",
    "                        #print(predict['setsData'][predict_index[slave_index[0]-1]][i])\n",
    "                        #print(predict['setsData'][predict_index[slave_index[0]-1]][i][x])\n",
    "                        #print(len(predict['setsData'][predict_index[slave_index[0]-1]][i][x]))\n",
    "                        temp_arr.append(predict['setsData'][predict_index[slave_index[0]-1]][i][x])\n",
    "                    \n",
    "                    \n",
    "                    #print(result['setsData'][index])\n",
    "                    result['setsData'][train_index[index]].append(temp_arr)\n",
    "                #print(result['setsData'][index])    \n",
    "                print(\"done\")\n",
    "    #print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
